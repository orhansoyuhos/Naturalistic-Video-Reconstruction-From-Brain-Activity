{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import Trainer\n",
    "from mxnet.gluon.loss import Loss, SigmoidBCELoss\n",
    "from mxnet.gluon.nn import BatchNorm, Conv2D, HybridSequential, LeakyReLU\n",
    "from mxnet.ndarray import NDArray, concat, full, mean, random_normal, stack, zeros\n",
    "from mxnet.ndarray.random import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self._capacity = capacity\n",
    "        self._data = []\n",
    "\n",
    "    def __call__(self, z_prime: NDArray) -> NDArray:\n",
    "        z = []\n",
    "\n",
    "        for i in range(z_prime.shape[0]):\n",
    "            if len(self._data) < self._capacity:\n",
    "                z.append(z_prime[i])\n",
    "                self._data.append(z_prime[i])\n",
    "            elif uniform().asscalar() < 0.5:\n",
    "                z.append(self._data.pop(randint(0, self._capacity).asscalar()))\n",
    "                self._data.append(z_prime[i])\n",
    "            else:\n",
    "                z.append(z_prime[i])\n",
    "\n",
    "        return stack(*z, axis=0)\n",
    "\n",
    "    @property\n",
    "    def capacity(self) -> int:\n",
    "        return self._capacity\n",
    "\n",
    "    @property\n",
    "    def data(self) -> List[NDArray]:\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lossfun:\n",
    "    def __init__(self, alpha: float) -> None:\n",
    "        self._alpha = alpha\n",
    "        self._bce = SigmoidBCELoss()\n",
    "\n",
    "    def __call__(self, p: float, p_hat: NDArray) -> NDArray:\n",
    "        return self._alpha * mean(self._bce(p_hat, full(p_hat.shape, p)))\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> float:\n",
    "        return self._alpha\n",
    "\n",
    "    @property\n",
    "    def bce(self) -> Loss:\n",
    "        return self._bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(HybridSequential):\n",
    "    def __init__(self, count: int, depth: int) -> None:\n",
    "        super(HybridSequential, self).__init__()\n",
    "\n",
    "        self._count = count\n",
    "        self._depth = depth\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.add(Conv2D(64, 4, 2, 1, in_channels=depth))\n",
    "            self.add(LeakyReLU(0.2))\n",
    "            self.add(Conv2D(128, 4, 2, 1, use_bias=False, in_channels=64))\n",
    "            self.add(BatchNorm(momentum=0.1, in_channels=128))\n",
    "            self.add(LeakyReLU(0.2))\n",
    "            self.add(Conv2D(256, 4, 2, 1, use_bias=False, in_channels=128))\n",
    "            self.add(BatchNorm(momentum=0.1, in_channels=256))\n",
    "            self.add(LeakyReLU(0.2))\n",
    "            self.add(Conv2D(512, 4, padding=1, use_bias=False, in_channels=256))\n",
    "            self.add(BatchNorm(momentum=0.1, in_channels=512))\n",
    "            self.add(LeakyReLU(0.2))\n",
    "            self.add(Conv2D(count, 3, 2, padding=1, in_channels=512))\n",
    "\n",
    "        for param in self.collect_params().values():\n",
    "            param.initialize()\n",
    "            if \"bias\" in param.name:\n",
    "                param.set_data(zeros(param.data().shape))\n",
    "            elif \"gamma\" in param.name:\n",
    "                param.set_data(random_normal(1, 0.02, param.data().shape))\n",
    "            elif \"weight\" in param.name:\n",
    "                param.set_data(random_normal(0, 0.02, param.data().shape))\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return self._count\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        return self._depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, input_channels) -> None:\n",
    "        self._history = History(50)\n",
    "        self._lossfun = Lossfun(1)\n",
    "        self._network = Network(1, input_channels+3)\n",
    "        self._trainer = Trainer(self._network.collect_params(), \"adam\", {\"beta1\": 0.5, \"learning_rate\": 0.0002})\n",
    "        # {\"beta1\": 0.5, \"learning_rate\": 0.0002}\n",
    "\n",
    "    @property\n",
    "    def history(self) -> History:\n",
    "        return self._history\n",
    "\n",
    "    @property\n",
    "    def lossfun(self) -> Lossfun:\n",
    "        return self._lossfun\n",
    "\n",
    "    @property\n",
    "    def network(self) -> HybridSequential:\n",
    "        return self._network\n",
    "\n",
    "    @property\n",
    "    def trainer(self) -> Trainer:\n",
    "        return self._trainer\n",
    "\n",
    "    def train(self, g: HybridSequential, x: NDArray, y: NDArray) -> float:\n",
    "        z = self._history(concat(x, g(x), dim=1))\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = 0.5 * (self.lossfun(0, self._network(z)) + self.lossfun(1, self._network(concat(x, y, dim=1))))\n",
    "\n",
    "        loss.backward()\n",
    "        self.trainer.step(1)\n",
    "\n",
    "        return float(loss.asscalar())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
