{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import Trainer\n",
    "from mxnet.gluon.loss import L1Loss, Loss, SigmoidBCELoss\n",
    "from mxnet.gluon.nn import Activation, BatchNorm, Conv2D, Conv2DTranspose, Dropout, HybridBlock, HybridSequential, LeakyReLU\n",
    "from mxnet.ndarray import NDArray, concat, full, mean, random_normal, zeros\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lossfun:\n",
    "\n",
    "    def __init__(self, alpha: float, beta_vgg:float, beta_pix: float, context=None) -> None:\n",
    "        self._alpha = alpha\n",
    "        self._bce = SigmoidBCELoss()\n",
    "        self._beta_vgg = beta_vgg\n",
    "        self._beta_pix = beta_pix\n",
    "        self._l1 = L1Loss()\n",
    "        self._vgg = VggLoss(context)\n",
    "    \n",
    "\n",
    "    def __call__(self, p: float, p_hat: NDArray, y: NDArray, y_hat: NDArray) -> NDArray:\n",
    "        \n",
    "        dis_loss = self._alpha * mean(self._bce(p_hat, full(p_hat.shape, p))) \n",
    "        \n",
    "        gen_loss_vgg = self._beta_vgg * mean(self._vgg(y_hat, y))\n",
    "        gen_loss_pix = self._beta_pix * mean(self._l1(y_hat, y))\n",
    "        \n",
    "        return dis_loss + gen_loss_vgg + gen_loss_pix\n",
    "                                             \n",
    "    \n",
    "    @property\n",
    "    def alpha(self) -> float:\n",
    "        return self._alpha\n",
    "\n",
    "    @property\n",
    "    def bce(self) -> Loss:\n",
    "        return self._bce\n",
    "\n",
    "    @property\n",
    "    def beta(self) -> float:\n",
    "        return self._beta\n",
    "\n",
    "    @property\n",
    "    def l1(self) -> Loss:\n",
    "        return self._l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VggLoss():\n",
    "    \n",
    "    def __init__(self, context) -> None:\n",
    "        #self.resizer_224 = mx.gluon.data.vision.transforms.Resize(224)\n",
    "        \n",
    "        self.vgg19=vision.vgg19(pretrained=True, ctx = context)\n",
    "        self.vgg_layer = 22\n",
    "        self._l1 = L1Loss()\n",
    "\n",
    "        \n",
    "    def __call__(self, y_hat, y):\n",
    "        #target_224 = self.resizer_224(y.transpose((0,2,3,1)))\n",
    "        #g_out_224 = self.resizer_224(y_hat.transpose((0,2,3,1)))\n",
    "        target_224 = mx.nd.contrib.BilinearResize2D(y, height=224, width=224)\n",
    "        g_out_224 = mx.nd.contrib.BilinearResize2D(y_hat, height=224, width=224)\n",
    "\n",
    "        feat_target = self.vgg19.features[:self.vgg_layer](target_224)\n",
    "        feat_out = self.vgg19.features[:self.vgg_layer](g_out_224)\n",
    "                      \n",
    "        return self._l1(feat_out, feat_target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(HybridBlock):\n",
    "    def __init__(self) -> None:\n",
    "        super(Layer, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def hybrid_forward(self, f: Any, x: NDArray, **kwargs) -> NDArray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Layer):\n",
    "    def __init__(self, count: int, depth: int) -> None:\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "        self._count = count\n",
    "        self._depth = depth\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return self._count\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        return self._depth\n",
    "\n",
    "    def hybrid_forward(self, f: Any, x: NDArray, **kwargs) -> NDArray:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skip(Layer):\n",
    "    def __init__(self, count: int, depth: int, layer: Layer) -> None:\n",
    "        super(Skip, self).__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            self._block = HybridSequential()\n",
    "\n",
    "            self._block.add(Conv2D(layer.depth, 4, 2, 1, use_bias=False, in_channels=depth))\n",
    "            self._block.add(BatchNorm(momentum=0.1, in_channels=layer.depth))\n",
    "            self._block.add(LeakyReLU(0.2))\n",
    "            self._block.add(layer)\n",
    "            self._block.add(Conv2DTranspose(count, 4, 2, 1, use_bias=False, in_channels=layer.count))\n",
    "            self._block.add(BatchNorm(momentum=0.1, in_channels=count))\n",
    "\n",
    "        self._count = count\n",
    "        self._depth = depth\n",
    "        self._layer = layer\n",
    "\n",
    "    @property\n",
    "    def block(self) -> HybridSequential:\n",
    "        return self._block\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return self._count + self._depth\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        return self._depth\n",
    "\n",
    "    @property\n",
    "    def layer(self) -> Layer:\n",
    "        return self._layer\n",
    "\n",
    "    def hybrid_forward(self, f: Any, x: NDArray, **kwargs) -> NDArray:\n",
    "        return f.relu(f.concat(x, self._block(x), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(HybridSequential):\n",
    "    def __init__(self, count: int, depth: int) -> None:\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self._count = count\n",
    "        self._depth = depth\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.add(Conv2D(64, 4, 2, 1, in_channels=depth))\n",
    "            self.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "            layer = Identity(512, 512)\n",
    "            layer = Skip(512, 512, layer)\n",
    "\n",
    "            for _ in range(0):\n",
    "                layer = Skip(512, 512, layer)\n",
    "\n",
    "                layer.block.add(Dropout(0.5))\n",
    "\n",
    "            layer = Skip(256, 256, layer)\n",
    "            layer = Skip(128, 128, layer)\n",
    "            layer = Skip(64, 64, layer)\n",
    "\n",
    "            self.add(layer)\n",
    "            self.add(Conv2DTranspose(count, 4, 2, 1, in_channels=128))\n",
    "            self.add(Activation(\"sigmoid\"))\n",
    "\n",
    "        for param in self.collect_params().values():\n",
    "            param.initialize()\n",
    "            if \"bias\" in param.name:\n",
    "                param.set_data(zeros(param.data().shape))\n",
    "            elif \"gamma\" in param.name:\n",
    "                param.set_data(random_normal(1, 0.02, param.data().shape))\n",
    "            elif \"weight\" in param.name:\n",
    "                param.set_data(random_normal(0, 0.02, param.data().shape))\n",
    "\n",
    "    @property\n",
    "    def count(self) -> int:\n",
    "        return self._count\n",
    "\n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        return self._depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_prediction:\n",
    "    def __init__(self, input_channels, context) -> None:\n",
    "        self._network = Network(3, input_channels)\n",
    "\n",
    "    @property\n",
    "    def network(self) -> HybridSequential:\n",
    "        return self._network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, input_channels, context) -> None:\n",
    "#         self._lossfun = Lossfun(1, 100)\n",
    "        self._lossfun = Lossfun(alpha= 1, beta_vgg=100, beta_pix= 1, context=context)\n",
    "        self._network = Network(3, input_channels)\n",
    "        self._trainer = Trainer(self._network.collect_params(), \"adam\", {\"beta1\": 0.5, \"learning_rate\": 0.0002})\n",
    "        # {\"beta1\": 0.5, \"learning_rate\": 0.0002}\n",
    "\n",
    "    @property\n",
    "    def lossfun(self) -> Lossfun:\n",
    "        return self._lossfun\n",
    "\n",
    "    @property\n",
    "    def network(self) -> HybridSequential:\n",
    "        return self._network\n",
    "\n",
    "    @property\n",
    "    def trainer(self) -> Trainer:\n",
    "        return self._trainer\n",
    "\n",
    "    def train(self, d: HybridSequential, x: NDArray, y: NDArray) -> float:\n",
    "        with autograd.record():\n",
    "            loss = (lambda y_hat: self.lossfun(1, d(concat(x, y_hat, dim=1)), y, y_hat))(self._network(x))\n",
    "\n",
    "        loss.backward()\n",
    "        self.trainer.step(1)\n",
    "\n",
    "        return float(loss.asscalar())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
